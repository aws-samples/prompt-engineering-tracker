{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### License\n",
    "Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "\n",
    "SPDX-License-Identifier: MIT-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "This notebook is meant to be an example of how to use the source code for the prompt engineering tracker. The notebook creates a quick LangChain retriever, LLM, and conversationalRetrievalChain to show how the prompt engineering tracker stores relevant information about the inputs and outputs of the chain. \n",
    "\n",
    "You will need to update a few things in this notebook to get it to run including:\n",
    "* the path for the ChainLogCallback\n",
    "* the ID of the Amazon Kendra index (or create an alternate retriever if you are not using Amazon Kendra)\n",
    "* The question you ask the chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Installs\n",
    "Install the required libraries from the requirements.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from langchain.llms import Bedrock\n",
    "\n",
    "from langchain.llms import Bedrock\n",
    "import boto3\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import AmazonKendraRetriever\n",
    "\n",
    "from src.chain_log_callback import ChainLogCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create callback to log the aspects of the chain\n",
    "In the cell below, we create the chain log callback so that we can pass it when creating the chain itself.\n",
    "This callback is how we save information about the chain, such as the prompt template, temperature, question asked, rating, etc.\n",
    "Note that you can request user inputs in the form of ratings and comments by setting `request_rating` or `request_comments` to true.\n",
    "\n",
    "The user_name, experiment_name, and path will all determine the path and name of the CSV file. \n",
    "The user_name is helpful if you have multiple folks working on a project together - the system adds the user_name into the csv file name.\n",
    "The experiment_name is helpful so that you can run different types of experiments. Let's say you want to experiment with RAG chains versus with a conversational retrieval chain. You may want those output to different CSV files. Or maybe you start experimenting in another language. You can change the name of the experiment or add a suffix to it (e.g. rag_test_2) to keep this separate.\n",
    "The path is the path for where to save the CSV files. It may be helpful to create a folder for these to be output so that you dont have to manually git ignore each one. \n",
    "\n",
    "Remember that this notebook is meant to be a sample to show you how to use the chain log callback in any application with one of the supported chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = ChainLogCallback(output_csv=True, \n",
    "                            request_rating=False, \n",
    "                            request_comments=False, \n",
    "                            user_name=\"yourUserName\", \n",
    "                            experiment_name=\"experiment_name\", \n",
    "                            path=\"path/to/where/you/want/the/csv/files/\",\n",
    "                            input_keyword=\"question\", # this helps ensure the input_variable to the prompt template for the actual user's input is consistent across chains\n",
    "                            combine_all_actions_into_one_log=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create base components of chain\n",
    "In this section, we create the clients, retriever, prompt templates, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(\"bedrock-runtime\")\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you will need to update the cell below with the correct ID of your Amazon Kendra index. If you are not using Amazon Kendra, you will need to create an [alternate retriever](https://python.langchain.com/docs/modules/data_connection/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = AmazonKendraRetriever(index_id=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condense_qa_template = \"\"\"{chat_history}\n",
    "    Human:\n",
    "    Given the previous conversation and a follow up question below, rephrase the follow up question\n",
    "    to be a standalone question.\n",
    "\n",
    "    Follow Up Question: {question}\n",
    "    Standalone Question:\n",
    "\n",
    "    Assistant:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt_template =  \"\"\"\n",
    "            Human:\n",
    "            Your job is to answer the question below based on the conversation history and the context from the relevant documentation. If you don't know the answer from the context and history, say \"I don't know\"\n",
    "\n",
    "            <context>\n",
    "            {context}\n",
    "            </context>\n",
    "\n",
    "            Here is the human's question:\n",
    "            <question>\n",
    "            {question}\n",
    "            </question>\n",
    "\n",
    "            Assistant:\n",
    "            \"\"\"\n",
    "input_variables = [\"context\", \"question\"]\n",
    "rag_claude_prompt = PromptTemplate(template=rag_prompt_template, input_variables=input_variables)\n",
    "\n",
    "model_id = \"anthropic.claude-instant-v1\"\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the LLM\n",
    "Note that the instantiation of the LLM includes the callback for the LLM. While we have included it here for reference, the code does not do much with the `on_llm_start` method, which is called when the LLM itself starts. Instead, the callback focuses on the chain actions which are more relevant to the chain use cases. \n",
    "\n",
    "Here, we use the Bedrock LLM and pass some various inputs to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = Bedrock(\n",
    "    model_kwargs={\"max_tokens_to_sample\":350,\"temperature\":0.1,\"top_k\":250,\"anthropic_version\":\"bedrock-2023-05-31\"},\n",
    "    model_id=model_id,\n",
    "    client=bedrock_client,\n",
    "    # verbose=True,\n",
    "    callbacks=[callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the chain\n",
    "When you create the chain, adding the custom callback created early in the notebook to the chain is what enables the logger to run. Without the callback, none of the inputs or outputs will be saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, verbose=True,\n",
    "    retriever=retriever,\n",
    "    # combine_docs_chain_kwargs={'prompt': rag_claude_prompt},\n",
    "    condense_question_prompt=PromptTemplate.from_template(condense_qa_template),\n",
    "    memory=ConversationBufferMemory(memory_key=\"chat_history\"),\n",
    "    callbacks=[callback]  # This is how you reference the chain log callback when creating the chain.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the chain\n",
    "Here we run the chain. A CSV should be created / updated with various inputs and outputs of the chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"YOUR_QUESTION_HERE\"\n",
    "chain = qa({\"question\": question, \"chat_history\": []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the answer if desired\n",
    "chain['answer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
